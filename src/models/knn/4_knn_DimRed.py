"""@author: marcosesquivelgonzalezEn este script buscamos reducciones dimensionales con el objetivo de evitar posibles problemas con alta dimensionalidad.Además, el buen resultado visto con las merged columns motiva también esta reducción de la dimensionalidad.Primero, se probará con PCA. PCA está diseñado para ser ejecutado realizando una estandarización previa.  De esta manera, todas las variablestienen la misma importancia en el PCA. Sin embargo, también se probará con el robustScaler en vista de sus buenos resultados. Se pedirá que el número de componentes encontradas tengan al menos el 90 % de la varianza total de los datos. Nota:Para el RobustScaler se llegará a este porcentaje con menor número de componentes al estar ponderadas las variables de manera diferente,se debería de escoger un umbral mayor para este caso)Después, se prueba con LDA/QDA basándonos en las merged columns encontradas en el EDA. Sin embargo, al contrario de lo que se entendía,LDA solo puede funcionar reduciendo los datos a 1 dimensión al tener solo dos clases. Esto provocará mucha perdida de información en el preprocesamiento, aún así se prueba.Se intentó implementar un PCA robusto a outlier: Trimmed Grassman Averages sin éxitohttps://github.com/glennq/tgahttps://openaccess.thecvf.com/content_cvpr_2014/papers/Hauberg_Grassmann_Averages_for_2014_CVPR_paper.pdf"""import osimport sysMODELS = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))SRC = os.path.abspath(os.path.join(MODELS, os.pardir))sys.path.append(SRC)import utilsimport numpy as npimport pandas as pdfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import cross_validatefrom sklearn.model_selection import GridSearchCVfrom sklearn.preprocessing import RobustScalerfrom sklearn.preprocessing import StandardScalerfrom sklearn.preprocessing import Normalizerfrom sklearn.decomposition import PCA,KernelPCAfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisimport TGAfrom R_PCA import R_pca,components_of_RPCAtrain_raw = utils.load_train()train_X = utils.one_hot_encode(df = train_raw.drop(["Transported", "PassengerId"], axis = 1))train_y = train_raw.Transportedprint('CV Score, Optimum_K, p')#------------Normalizer scaler------------------NormScal = Normalizer()colnames = train_X.columnsdf_array = NormScal.fit_transform(train_X)train_X_NormScaled = pd.DataFrame(df_array,columns=colnames)#-------------- Robust scaler ------------------------RobScal = RobustScaler()colnames = train_X.columnsdf_array = RobScal.fit_transform(train_X)train_X_RobScaled = pd.DataFrame(df_array,columns=colnames)#---------------- Standard Scaler------------------StandScal = StandardScaler()colnames = train_X.columnsdf_array = StandScal.fit_transform(train_X)train_X_SdScaled = pd.DataFrame(df_array,columns=colnames)#------------------------------PCA's con RobustScaler-------------------------------print('\n PCA con RobustScaler:\n')pca_RobS = PCA(n_components=train_X.shape[1],random_state=1)pca_RobS.fit(train_X_RobScaled)ratios = pca_RobS.explained_variance_ratio_for i in range(train_X.shape[1]):  if np.cumsum(ratios)[i] >= 0.9:    num_pca_RobS = i + 1    print ("El número mínimo de PCA's para RobustScaler llegue al 90% de varianza: {}".format(num_pca_RobS))    breaktrain_X_RobS_PCA = pca_RobS.transform(train_X_RobScaled)for components in range(num_pca_RobS,20):        train_comp = train_X_RobS_PCA[:,:components]    print(" ·",components,'components, explained variance:',np.round(np.cumsum(pca_RobS.explained_variance_ratio_)[components-1],7))    for p_ in [1,2]:        optim_score = 0        for k in range(10,50):                score_cv = cross_validate(estimator = KNeighborsClassifier(n_neighbors=k,p = p_),                                      X = train_comp, y = train_y, cv = 10, n_jobs = -1)                        score_test = np.mean(score_cv["test_score"])                        if score_test > optim_score:                optim_score = score_test                k_optim = k                p_optim = p_                print("%.5f,%.d,%.d"%(optim_score,k_optim, p_optim))                #------------------------------PCA's con StandardScaler-------------------------------print('\n PCA con StandardScaler:\n')pca_SS = PCA(n_components=train_X.shape[1], random_state = 1)pca_SS.fit(train_X_SdScaled)ratios = pca_SS.explained_variance_ratio_for i in range(train_X.shape[1]):  if np.cumsum(ratios)[i] >= 0.9:    num_pca_SS = i + 1    print ("El número mínimo de PCA's para StandardScaler llegue al 90% de varianza: {}\n".format(num_pca_SS))    breaktrain_X_SS_PCA = pca_SS.transform(train_X_SdScaled)for components in range(num_pca_SS,20):        train_comp = train_X_SS_PCA[:,:components]    print(" ·",components,'components, explained variance:',np.round(np.cumsum(pca_SS.explained_variance_ratio_)[components-1],7))    for p_ in [1,2]:        optim_score = 0        for k in range(10,50):                score_cv = cross_validate(estimator = KNeighborsClassifier(n_neighbors=k,p = p_),                                      X = train_comp, y = train_y, cv = 10, n_jobs = -1)                        score_test = np.mean(score_cv["test_score"])                        if score_test > optim_score:                optim_score = score_test                k_optim = k                p_optim = p_                print("%.5f,%.d,%.d"%(optim_score,k_optim, p_optim))#------------------------------Kernel_PCA_StandardScaler----------------------------------print('\n KPCA with polynomial kernel y StandardScaler:\n')KPCA = KernelPCA(kernel='poly',fit_inverse_transform=True)KPCA.fit(train_X_SdScaled)evals = KPCA.eigenvalues_explained_var_prop = np.cumsum(evals)/np.sum(evals)for i in range(train_X.shape[1]):  if explained_var_prop[i] >= 0.9:    num_kpca_SS = i + 1    print ("El número mínimo de KPCA's para StandardScaler llegue al 90% de varianza: {}\n".format(num_kpca_SS))    breaktrain_X_SS_Pol_PCA = KPCA.X_transformed_fit_for components in range(num_kpca_SS,20):    train_comp = train_X_SS_Pol_PCA[:,:components]    print(" ·",components,'components, explained variance:',explained_var_prop[components-1])    for p_ in [1,2]:         optim_score = 0        for k in range(10,50):            score_cv = cross_validate(estimator = KNeighborsClassifier(n_neighbors=k,p = p_),                                              X = train_comp, y = train_y, cv = 10, n_jobs = -1)                                score_test = np.mean(score_cv["test_score"])                                if score_test > optim_score:                optim_score = score_test                k_optim = k                p_optim = p_        print("%.5f,%.d,%.d"%(optim_score,k_optim, p_optim))#------------------------------Kernel_PCA_NormalizerScaler----------------------------------print('\n KPCA with polynomial kernel y NormalizerScaler:\n')#Robust Scaler encontraba una matriz singular y no se podía resolverKPCA = KernelPCA(kernel='poly',fit_inverse_transform=True,n_components=20)KPCA.fit(train_X_NormScaled)evals = KPCA.eigenvalues_explained_var_prop = np.cumsum(evals)/np.sum(evals)for i in range(train_X.shape[1]):  if explained_var_prop[i] >= 0.9:    num_kpca_NormS = i + 1    print ("El número mínimo de KPCA's para RobustScaler llegue al 90% de varianza: {}\n".format(num_kpca_NormS))    breaktrain_X_NormS_Pol_PCA = KPCA.X_transformed_fit_for components in range(num_kpca_NormS,20):    train_comp = train_X_NormS_Pol_PCA[:,:components]    print(" ·",components,'components, explained variance:',explained_var_prop[components-1])    for p_ in [1,2]:           optim_score = 0        for k in range(10,50):            score_cv = cross_validate(estimator = KNeighborsClassifier(n_neighbors=k,p = p_),                                              X = train_comp, y = train_y, cv = 10, n_jobs = -1)                                score_test = np.mean(score_cv["test_score"])                                if score_test > optim_score:                optim_score = score_test                k_optim = k                p_optim = p_        print("%.5f,%.d,%.d"%(optim_score,k_optim, p_optim))#----------------------------LDA con RobustScaler----------------------print('\n LDA con RobustScaler:')lda_RS = LinearDiscriminantAnalysis()   train_X_LDA = lda_RS.fit_transform(train_X_RobScaled,train_y)optim_score = 0for p_ in [1,2]:       for k in range(1,100):        score_cv = cross_validate(estimator = KNeighborsClassifier(n_neighbors=k,p = p_),                                          X = train_X_LDA, y = train_y, cv = 10, n_jobs = -1)                        score_test = np.mean(score_cv["test_score"])                        if score_test > optim_score:            optim_score = score_test            k_optim = k            p_optim = p_        print("%.5f,%.d,%.d"%(optim_score,k_optim, p_optim))#----------------------------LDA con StandardScaler----------------------print('\n LDA con StandardScaler:\n')lda_SS = LinearDiscriminantAnalysis()   train_X_LDA = lda_SS.fit_transform(train_X_SdScaled,train_y)optim_score = 0for p_ in [1,2]:       for k in range(1,100):        score_cv = cross_validate(estimator = KNeighborsClassifier(n_neighbors=k,p = p_),                                          X = train_X_LDA, y = train_y, cv = 10, n_jobs = -1)                        score_test = np.mean(score_cv["test_score"])                        if score_test > optim_score:            optim_score = score_test            k_optim = k            p_optim = p_        print("%.5f,%.d,%.d"%(optim_score,k_optim, p_optim))#------------------------R-PCA---------------------print('\nRobustPCA:\n')rpca = R_pca(train_X)L, S = rpca.fit(max_iter=10000,iter_print=200)#L es el Low-Rank data matrix, conserva las características principales de los datosoptim_score = 0for ncomp in range(2,17):    pca = PCA(n_components = ncomp, random_state=1)    Data = pca.fit_transform(L)    print(" ·",ncomp,'components, explained variance:',np.round(np.cumsum(pca.explained_variance_ratio_)[ncomp-1],7))    for p_ in [1,2]:           for k in range(1,100):            score_cv = cross_validate(estimator = KNeighborsClassifier(n_neighbors=k,p = p_),                                              X = Data, y = train_y, cv = 10, n_jobs = -1)                                score_test = np.mean(score_cv["test_score"])                                if score_test > optim_score:                optim_score = score_test                k_optim = k                p_optim = p_        print("%.5f,%.d,%.d"%(optim_score,k_optim, p_optim))"""#Predicciones en el testtest_raw = utils.load_test()test = utils.one_hot_encode(df = test_raw.drop(["PassengerId"], axis = 1))colnames = test.columnstest_scaled_array = RobScal.transform(test)test_scaled = pd.DataFrame(test_scaled_array,columns=colnames)knn = KNeighborsClassifier(n_neighbors = k_optim ,p = p_optim).fit(train_X_scaled,train_y)pred_labels = knn.predict(X = test_scaled)predicted_labels = utils.encode_labels(pred_labels)#utils.generate_submission(labels = predicted_labels, method = "knn", notes = "RobScal_k_" + str(k_optim) + "_and_p_" + str(p_optim))"""